#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Wed Dec 19 12:07:36 2018

@author: Florian Krempl

Capstone Project: Predicting Transit time with Machine Learning
Partner Company: DAMCO/Maersk

"""

import os
import numpy as np
import pandas as pd

# set working directory
os.chdir('E:\MIT\Capstone')

# create empty dataframe to append loaded csv data later
customer_clean = pd.DataFrame({'Shipper':[], 'Carrier':[],
                                'Original Port Of Loading':[],
                                'Original Port Of Loading Site':[],
                                'Final Port Of Discharge':[],
                                'Final Port Of Discharge Site':[],
                                'Origin Service':[], 
                                'Book Date':[], 'Expected Receipt Date':[], 'Actual Receipt Date': [], 
                                'Gate In Origin-Actual':[], 'Consolidation Date':[],
                                'ETD':[], 'ATD':[],'ETA':[], 'ATA':[],
                                'Container Unload From Vessel-Actual':[],
                                'Equipment Number':[]})

# list of all column names - to make sure all documents have the same col names
column_names = ['PO Line Uploaded', 'POH Client Date', 'POH Upload Date',
                'Book Date', 'Receipt Date', 'Consolidation Date',
                'ETD', 'ETA', 'ATD', 'ATA', 'Consignee',
                'PO Number', 'Origin Service', 'Destination Service', 'Consignee.1',
                'Carrier', 'VOCC Carrier', 'Carrier SCAC', 'CBL Number',
                'Booking Number', 'Shipper', 'Supplier', 'Buyer', 'Seller',
                'Original Port Of Loading', 'Original Port Of Loading Site',
                'Final Port Of Discharge', 'Final Port Of Discharge Site',
                'Actual Measurement', 'Earliest Receipt Date', 'Expected Receipt Date',
                'Latest Receipt Date', 'Actual Receipt Date',
                'Empty Equipment Dispatch-Actual', 'Gate In Origin-Actual',
                'Container Loaded On Vessel-Actual', 'Consolidation Date.1',
                'Container Unload From Vessel-Actual', 'Gate Out Destination-Actual',
                'Container Empty Return-Actual', 'Equipment Number',
                'Confirmation Date']

# list of customers from which we got data
customers = ['USWA', 'USAD', 'USNI', 'USHO', 'USTA',
             'USDO', 'USCL', 'USHA', 'USHE']
# customer_name = 'USTE' does not work

for customer_name in customers:
    # load csv files
    print("start to load", customer_name, "...") 
    customer = pd.read_csv('data/Capstone Project data/' + customer_name +'.csv', encoding= 'latin1')
    print("loaded", customer_name, "starting to clean the dataframe")
    # test for unnecessary columns
    if customer.columns[0] == 'Unnamed: 0':
        customer = customer.drop(columns='Unnamed: 0')
        print('droped unnecessary index column')
    # get right column names    
    customer.columns = column_names
    # get real carrier
    customer["Carrier"] = np.where(customer['VOCC Carrier'].isna,
            customer["CBL Number"],    #"Carrier SCAC"
            customer["VOCC Carrier"])
    # get rid of missing data - select right columns
    customer = (customer.loc[customer['Final Port Of Discharge Site'] == 'UNITED STATES']
                        .loc[customer['ETD'].notna()]
                        .loc[customer['Gate In Origin-Actual'].notna()]
                        .loc[customer['Container Unload From Vessel-Actual'].notna()]
                        .loc[:,['Shipper', 'Carrier',
                                'Original Port Of Loading',
                                'Original Port Of Loading Site',
                                'Final Port Of Discharge',
                                'Final Port Of Discharge Site',
                                'Origin Service', 
                                'Book Date', 'Expected Receipt Date', 'Actual Receipt Date',
                                'Gate In Origin-Actual', 'Consolidation Date',
                                'ETD', 'ATD','ETA', 'ATA',
                                'Container Unload From Vessel-Actual',
                                'Equipment Number']])
    # attach customer name column
    customer['customer'] = customer_name
    # customer['Final Port Of Discharge'] = customer['Final Port Of Discharge'].str.title()
    # append to customer_clean
    print("appending", customer_name, "to customer_clean", "\n") 
    customer_clean = customer_clean.append(customer)
# get new index - let's start clean from here
customer_clean = customer_clean.reset_index(drop=True)

# define date columns for formatting
date_columns = ['Book Date','Gate In Origin-Actual', 'Actual Receipt Date',
                'Container Unload From Vessel-Actual',
                'ETD', 'Expected Receipt Date',
                'ATA', 'ATD', 'ATA', 'ETA']
# make sure date is in the same format
customer_clean[date_columns] = customer_clean[date_columns].replace('-', '/', regex = True)

# delete wrong date formats - this needs some work - will rewrite this soon
customer_clean = customer_clean.drop(customer_clean[customer_clean['Book Date'].str.slice(6,10) > '2500'].index)
customer_clean = customer_clean.drop(customer_clean[customer_clean['Book Date'].str.contains('3016|6186|2528|7340|0006')].index)

# get date to right format:
for column in date_columns:
    print(['starting converting column', column, 'to date'])
    customer_clean[column] = pd.to_datetime(customer_clean[column], format =  '%d/%m/%Y')

# get y columns
customer_clean['y_gate'] = customer_clean['Container Unload From Vessel-Actual']-customer_clean['Gate In Origin-Actual']
customer_clean['y_gate'] = customer_clean['y_gate'].dt.days

customer_clean['y_book'] = customer_clean['Container Unload From Vessel-Actual']-customer_clean['Book Date']
customer_clean['y_book'] = customer_clean['y_book'].dt.days

customer_clean['y_receive'] = customer_clean['Container Unload From Vessel-Actual']-customer_clean['Actual Receipt Date']
customer_clean['y_receive'] = customer_clean['y_receive'].dt.days

customer_clean['y_depart'] = customer_clean['Container Unload From Vessel-Actual']-customer_clean['ATD']
customer_clean['y_depart'] = customer_clean['y_depart'].dt.days

print('finished calculating y columns')
# get weekday of planned departure
customer_clean['weekday'] = customer_clean['ETD'].dt.weekday

# get day of year of arrival date
customer_clean['doy'] = customer_clean['ETA'].dt.dayofyear

# get consolidation columns
customer_clean['consolidation'] = np.where(customer_clean['Origin Service'] == 'CFS', 1, 0)

# get estimated time to receival of goods
customer_clean['time_to_port'] = customer_clean['Expected Receipt Date'] - customer_clean['Book Date']
customer_clean['time_to_port'] = customer_clean['time_to_port'].dt.days

# get late departure from port of origin
customer_clean['late departure'] = customer_clean['ATD'] - customer_clean['ETD']
customer_clean['late departure'] = customer_clean['late departure'].dt.days

print('finished calculating various predictor variables')
######################
## load statistics  ##
######################
# this whole next part should be a fancy SQL query on the company database - our data is in csv format - so...

# load port of origin statistics
port_origin = pd.read_csv('data/statistics/summary_port_of_origin.csv',
                          sep = ',', encoding= 'latin1')
port_origin = port_origin[['Shipper','Original Port Of Loading','std','median']]
port_origin.columns = ['Shipper','Original Port Of Loading','std_po','median_po']

# load chinese new year dates
chinese_holidays = pd.read_csv('data/statistics/chinese_holidays_complete.csv',
                               sep = ',', encoding= 'latin1')
chinese_holidays['date'] = pd.to_datetime(chinese_holidays['date'])

# load route carrier statistics
route = pd.read_csv('data/statistics/summary_route_carrier.csv',
                    sep = ',', encoding = 'latin1')
route = route[['Carrier', 'Original Port Of Loading', 'Final Port Of Discharge',
               'std', 'median']]
route.columns = ['Carrier', 'Original Port Of Loading', 'Final Port Of Discharge',
                 'std_route', 'median_route']

# load carrier schedule statistics
route_schedule = pd.read_csv('data/statistics/summary_route_carrier_schedule.csv',
                             sep = ',', encoding='latin1')
route_schedule = route_schedule[['Carrier', 'Original Port Of Loading',
                                 'Final Port Of Discharge', 'weekday', 'mean_schedule']]

# load port of destination carrier statistics
port_dest = pd.read_csv('data/statistics/summary_port_of_destination.csv',
                        sep = ',', encoding='latin1')
port_dest = port_dest[['customer', 'Carrier', 'Final Port Of Discharge',
                       'std_port_carrier', 'median']]
port_dest.columns = ['customer', 'Carrier', 'Final Port Of Discharge',
                     'std_pd', 'median_pd',]
port_dest['Final Port Of Discharge'] = port_dest['Final Port Of Discharge'].str.upper()

# load us port capacity statistics (this one is a little more complicated than necessary... will change soon)
port_cap = pd.read_csv('data/statistics/summary_ports_us.csv',
                       sep = ',', encoding='latin1')
port_cap = port_cap[['City', 'year', 'Arrival Date', 'cap']]
# get right date format
port_cap['Arrival Date'] = pd.to_datetime(port_cap['Arrival Date'], format = '%d.%m.%Y')
port_cap.index = port_cap['Arrival Date'].dt.dayofyear
# get rolling mean for 3 days for every year
port_cap = (port_cap.groupby(['City', 'year'])['cap']
                    .rolling(3, center = True, min_periods = 1).mean()
                    .reset_index())
port_cap.columns = ['City', 'year', 'doy', 'cap']
# get mean over all years
port_cap = port_cap.groupby(['City', 'doy'])['cap'].mean().reset_index()

# get right string format
port_cap['City'] = port_cap['City'].str.upper()

###############
## merge dfs ##
###############
print('merging dataframes')
customer_clean = (customer_clean.merge(port_origin,
                                      on = ['Shipper','Original Port Of Loading'],
                                      how = 'inner')
                                .merge(port_dest, 
                                       on = ['customer', 'Carrier', 'Final Port Of Discharge'],
                                       how = 'inner')
                                .merge(route,
                                       on = ['Carrier', 'Original Port Of Loading', 'Final Port Of Discharge'],
                                       how = 'left')
                                .merge(route_schedule,
                                       on = ['Carrier', 'Original Port Of Loading',
                                             'Final Port Of Discharge', 'weekday'],
                                       how = 'left',)
                                .merge(port_cap, 
                                       left_on = ['Final Port Of Discharge', 'doy'],
                                       right_on = ['City', 'doy'],
                                       how = 'left'))

# create column holiday for chinese holidays
customer_clean['holiday'] = np.where(np.isin(customer_clean['ETD'], chinese_holidays['date']), 1, 0)

# get missing values for column mean schedule - get it from normal median of route regardless the day
customer_clean['mean_schedule'] = np.where(np.isnan(customer_clean['mean_schedule']),
                                           customer_clean['median_route'],
                                           customer_clean['mean_schedule'])

# create column quarter of the year
customer_clean['quarter'] = customer_clean['ETD'].dt.quarter
customer_clean = customer_clean.join(pd.get_dummies(customer_clean['quarter']))
customer_clean = customer_clean.drop(['quarter', 4], axis = 1)

# create dummy columns for customers - drop last column
customer_clean = customer_clean.join(pd.get_dummies(customer_clean['customer']))
customer_clean = customer_clean.drop('USWA', axis=1)

# drop duplicates - only keep shipments which are unique to avoid overfitting on big customers / routes
print('dropping duplicated lines')
customer_clean = customer_clean.drop_duplicates(subset=['customer',
                                                        'Carrier', 'Shipper', 'Final Port Of Discharge', 
                                                        'Original Port Of Loading',
                                                        'Container Unload From Vessel-Actual',
                                                        'Gate In Origin-Actual', 'y_depart'])
                                

### get test performance function
from sklearn import metrics
def print_metrics(y_hat, y_test):
    output = print('MAE: ', round(np.mean(abs(y_hat-y_test)), 2), "\n",
                   'MAPE: ', round(np.mean(abs(y_hat-y_test)/y_test), 4), "\n",
                   'RMSE: ', round(np.sqrt(np.mean((y_test - y_hat)**2)), 2), "\n"
                   ' R2: ', round(metrics.r2_score(y_test, y_hat),2)
                   )
    return output

print('ready for training')
