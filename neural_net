#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Dec 18 10:59:16 2018

@author: Florian Krempl
"""

import numpy as np
import pandas as pd

from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from keras.wrappers.scikit_learn import KerasRegressor
from keras.models import model_from_json
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn import metrics
import matplotlib.pyplot as plt
import os


os.chdir('E:\MIT\Capstone')
os.getcwd()

pd.set_option('display.expand_frame_repr', False)
seed = 1992
np.random.seed(seed)

exec(open('scripts/capstone_datapipeline_cleaning').read())

# set up training and test set
X = customer_clean[['consolidation', 'std_po', 'median_po', 'holiday',
                    'std_pd', 'median_pd', 'cap',
                    'mean_schedule', 'std_route', 1, 2, 3,
                    'USAD', 'USDO', 'USHA', 'USHE', 'USHO', 'USTA', 'y']]

X = X.dropna()

y = X['y']

X = X.drop('y', axis = 1)

# standardize

num_columns = ['std_po', 'median_po', 'std_pd', 'median_pd',
               'cap','mean_schedule', 'std_route',]

scaler = StandardScaler(copy=True, with_mean=True, with_std=True)
scaler.fit(X[num_columns])
X.loc[:,num_columns] = scaler.transform(X[num_columns])

X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=1992)

#######################
# deep neural network #
#######################

model = Sequential()
# 1st layer
model.add(Dense(20, input_dim=len(X.columns),
                kernel_initializer='normal',
                activation='relu'))
model.add(Dropout(0.2))
# 2nd layer
model.add(Dense(20,
                kernel_initializer='normal',
                activation='relu'))
model.add(Dropout(0.2))
# output layer
model.add(Dense(1, kernel_initializer='normal'))
# compile model
model.compile(loss='mse', optimizer='adam', metrics = ['mape', 'mae'])

###############
# plot loss  ##
###############

history = model.fit(X_train, y_train, validation_split = 0.3,
                    epochs = 200, batch_size = 2000)

print(history.history.keys())
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()


# loss = 11
# train best model

model.fit(X, y, validation_split = 0.3,
          epochs = 1000, batch_size = 5000)

print(model.summary())


################
#  save model ##
################

# serialize model to JSON
model_json = model.to_json()
with open("data/models/model_1_1.json", "w") as json_file:
    json_file.write(model_json)
# serialize weights to HDF5
model.save_weights("data/models/model_1_1.h5")
print("Saved model to disk")
 
# later...
 
# load json and create model
json_file = open('data/models/model_1_1.json', 'r')
loaded_model_json = json_file.read()
json_file.close()
model = model_from_json(loaded_model_json)
# load weights into new model
model.load_weights("data/models/model_1_1.h5")
print("Loaded model from disk")



##############
## test nn  ##
##############


y_hat = model.predict(X_test)

test_data = pd.DataFrame(data = {'y_hat': np.NaN,
                                 'y_test': y_test})
test_data['y_hat'] = y_hat


def print_metrics(y_hat, y_test):
    output = print('ME: ', round(np.mean(y_test-y_hat), 2), "\n",
                   'MAE: ', round(np.mean(abs(y_test-y_hat)), 2), "\n",
                   'MAPE: ', round(np.mean(abs(y_test-y_hat)/y_test), 4), "\n",
                   'RMSE: ', round(np.sqrt(np.mean((y_test-y_hat)**2)), 2), "\n"
                   ' R2: ', round(metrics.r2_score(y_true = y_test,
                                                   y_pred = y_hat),2)
                   )
    return output

print_metrics(test_data['y_hat'], test_data['y_test'])

##################
## save results ##
##################

X_test['y_hat'] = y_hat
result = customer_clean.join(X_test['y_hat'], on=None)
result = result.dropna(subset=['y_hat'])
result.to_csv('data/results/result_randomforest.csv')
customer_clean.to_csv('data/results/customer_data.csv')
#view = result.head(50)
##########################
# hyperparameter tuning ##
##########################

def create_model(neurons=1, neurons_2=2):
    model = Sequential()
    # 1st layer
    model.add(Dense(neurons, input_dim=len(X.columns),
                    kernel_initializer='normal',
                    activation='relu'))
    model.add(Dropout(0.2))
    # 2nd layer
    model.add(Dense(neurons_2,
                    kernel_initializer='normal',
                    activation='relu'))
    model.add(Dropout(0.2))
#    # 3rd layer
#    model.add(Dense(neurons_3,
#                    kernel_initializer='normal',
#                    activation='relu'))
#    model.add(Dropout(0.2))
    # output layer
    model.add(Dense(1, kernel_initializer='normal'))
    # compile model
    model.compile(loss='mse', optimizer='adam', metrics = ['mape', 'mae'])
    return model


# validation split !!
model = KerasRegressor(build_fn = create_model,
                       epochs=1000, batch_size=2000,
                       verbose=2, validation_split = 0.3)

# define the grid search parameters
neurons = [18, 30]
neurons_2 = [18, 30]
neurons_3 = [5, 10, 20, 30]


param_grid = dict(neurons=neurons, neurons_2 = neurons_2)

# grid search
grid = GridSearchCV(estimator=model, param_grid=param_grid,
                    n_jobs=1, verbose=2)

grid_result = grid.fit(X, y)



print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))

y_hat = grid_result.predict(X_test)

print_metrics(y_hat, y_test)
grid_result.best_score_
grid_result.best_params_
grid_result.cv_results_['mean_test_score']

#############################
## test mean carrier route ##
#############################

customer_clean = customer_clean.merge(
                    (customer_clean.groupby(['Carrier',
                                             'Original Port Of Loading',
                                             'Final Port Of Discharge'])['y_book_y']
                                    .mean()
                                    .reset_index()),
                    on=['Carrier',
                        'Original Port Of Loading',
                        'Final Port Of Discharge'],
                    how='left')
    
customer_clean.columns
customer_clean.columns = ['ATA', 'ATD', 'Book Date', 'Carrier',
                          'Consolidation Date', 'Container Unload From Vessel-Actual',
                          'ETA', 'ETD', 'Equipment Number', 'Expected Receipt Date',
                          'Final Port Of Discharge', 'Final Port Of Discharge Site',
                          'Gate In Origin-Actual', 'Origin Service', 
                          'Original Port Of Loading', 'Original Port Of Loading Site',
                          'Shipper','customer', 'y', 'weekday', 'doy',
                          'consolidation','std_po','median_po','std_pd',
                          'median_pd','std_route','median_route','mean_schedule',
                          'City','cap','holiday',1,2,3,'USAD','USDO','USHA',
                          'USHE','USHO','USTA','y_mean']


print_metrics(customer_clean['y_mean'], customer_clean['y_book'])


####################################
##  multiple linear regression
####################################


from sklearn.linear_model import LinearRegression

lin_reg = LinearRegression(n_jobs = -1)

lin_reg.fit(X_train, y_train)

y_hat = lin_reg.predict(X_test)

print_metrics(y_hat, y_test)


